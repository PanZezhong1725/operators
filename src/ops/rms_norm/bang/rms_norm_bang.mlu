#include "bang.h"
#include "bang_device_functions.h"
#include "cnrt.h"
#include "rms_norm_bang.h"
#include "../../../devices/bang/common_bang.h"

const int NRAM_MAX_SIZE = 1024 * (512 + 128);//the maximum NRAM memory is 1024 * 768
const int nramNum = NRAM_MAX_SIZE/sizeof(float);
__nram__  float nram_buffer[nramNum];
const int SRC_MAX_SIZE = 1024 * 128;//至少大于等于128字节
const int maxNum = SRC_MAX_SIZE/sizeof(float); 
const int wSize = 32;//后续为了针对axis=-1进行规约求和
template <typename T>
__mlu_device__ void rmsNormKernel(T *source, T *destination, T *weight, int *strideSrc, int *strideDest, int *shape, int othersize, int dimsize, int dimS, float eps) {//axis=-1
    int segNum = dimS / wSize;
    if(dimsize >= maxNum){
        T *src = nram_buffer;//[maxNum]
        T *destSumFinal = src + maxNum;//[wSize]
        T *destSum = destSumFinal + wSize;//[wSize]
        T *wet = destSum + wSize;//[maxNum]
        
        int remain = dimsize % maxNum;
        int repeat = (dimsize - remain) / maxNum;
        int tidS;
        int tidD;
        for(int i = 0; i < othersize; i += taskDim){
            int inds = 0;
            int indd = 0;
            int indi = i + taskId;
            for (int j = nDim - 2; j >= 0; --j) {
                inds += (indi % shape[j]) * strideSrc[j];
                indd += (indi % shape[j]) * strideDest[j];
                indi /= shape[j];
            }
            __bang_write_zero(destSumFinal, wSize);
            for(int s = 0; s < repeat; s++){
                __bang_write_zero(destSum, wSize);
                tidS = inds + s * maxNum;
                __memcpy(src, source + tidS, maxNum * sizeof(T), GDRAM2NRAM);
                __bang_mul(src, src, src, maxNum);//src = src * src
                int segNum = maxNum / wSize;//准备数值求和
                for(int strip = segNum / 2; strip > 0; strip = strip / 2){
                    for(int j = 0; j < strip; j++){
                        __bang_add(src + j * wSize, src + j * wSize, src + (j + strip) * wSize, wSize);
                    }
                }
                __bang_reduce_sum(destSum, src, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                __bang_add(destSumFinal, destSumFinal, destSum, wSize);
            }
            
            if(remain){
                tidS = inds + repeat * maxNum;
                __bang_write_zero(src, maxNum);
                __memcpy(src, source + tidS, remain * sizeof(T), GDRAM2NRAM);
                __bang_mul(src, src, src, maxNum);//src = src * src
                int segNum = maxNum / wSize;//准备数值求和
                for(int strip = segNum / 2; strip > 0; strip = strip / 2){
                    for(int j = 0; j < strip; j++){
                        __bang_add(src + j * wSize, src + j * wSize, src + (j+ strip) * wSize, wSize);
                    }
                }
                __bang_reduce_sum(destSum, src, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                __bang_add(destSumFinal, destSumFinal, destSum, wSize);
            }
            
            destSumFinal[0] += eps;
            destSumFinal[0] /= dimsize;
            destSum[0] = pow(destSum[0], 0.5);
            T globalSumInv = 1.0 / destSumFinal[0];
            //-----------
            for(int s = 0; s < repeat; s++){
                tidS = inds + s * maxNum;
                tidD = indd + s * maxNum;
                __memcpy(src, source + tidS, maxNum * sizeof(T), GDRAM2NRAM);
                
                __memcpy(wet, weight + s * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                if(taskId == 0 && s == 0 && i == 0){
                    __bang_printf("%.4e, %.4e, %.4e, %.4e\n", globalSumInv, src[1], weight[1], wet[1]);
                }
                __bang_mul(src, src, wet, maxNum);//src = src * wet
                __bang_mul_scalar(src, src, globalSumInv, maxNum);
                __memcpy(destination + tidD, src, maxNum * sizeof(T), NRAM2GDRAM);
            }
            if(remain){
                tidS = inds + repeat * maxNum;
                tidD = indd + repeat * maxNum;
                __memcpy(src, source + tidS, remain * sizeof(T), GDRAM2NRAM);
                __memcpy(wet, weight + repeat * maxNum, remain * sizeof(T), GDRAM2NRAM);
                __bang_mul(src, src, wet, maxNum);//src = src * wet
                __bang_mul_scalar(src, src, globalSumInv, maxNum);
                __memcpy(destination + tidD, src, remain * sizeof(T), NRAM2GDRAM); 
            }
        }
    }
    else{//dimsize < maxNum
        T *src = nram_buffer;
        T *wet = src + dimsize;   
        T *destSum = wet + dimsize;  
        T *destSumFinal = destSum + dimS;
        
        __bang_write_zero(destSum, dimS);
        __memcpy(wet, weight, dimsize * sizeof(T), GDRAM2NRAM);
        for(int i = 0; i < othersize; i += taskDim){
            int inds = 0;
            int indd = 0;
            int indi = i + taskId;
            for (int j = nDim - 2; j >= 0; --j) {
                inds += (indi % shape[j]) * strideSrc[j];
                indd += (indi % shape[j]) * strideDest[j];
                indi /= shape[j];
            }
            __memcpy(src, source + inds, dimsize * sizeof(T), GDRAM2NRAM);
            __bang_mul(destSum, src, src, dimsize);//src = src * src
            int segNum = dimS / wSize;
            for(int strip = segNum / 2; strip > 0; strip = strip / 2){
                for(int j = 0; j < strip; j++){
                    __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                }
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);
            destSumFinal[0] += eps;
            destSumFinal[0] /= dimsize;
            destSumFinal[0] = pow(destSum[0], 0.5);
            T globalSumInv = 1.0 / destSum[0];
            __bang_mul(src, src, wet, dimsize);
            __bang_mul_scalar(src, src, globalSumInv, dimsize);
            __memcpy(destination + indd, src, dimsize * sizeof(T), NRAM2GDRAM);
        }
    }
}

template<typename T>
__mlu_global__ void rmsNormUnion1(T *mlu_destination, T const *mlu_src, T const *mlu_weight, int *strideSrc, int *strideDest, int *shape, int othersize, int dimsize, int dimS, float eps) {

    rmsNormKernel<T>(mlu_src, mlu_destination, mlu_weight, strideSrc, strideDest, shape, othersize, dimsize, dimS, eps);
}

template<typename T>
void rmsNorm(cnrtQueue_t queue, void *y, void const *x, void const *w, int *strideSrc, int *strideDest, int *shape, int n, int d, float eps) {
    const int wSize = 128 / sizeof(T);
    auto y_ = reinterpret_cast<T *>(y);
    auto x_ = reinterpret_cast<T const *>(x);
    auto w_ = reinterpret_cast<T const *>(w);

    int dimS;
    float mi = log2(d);
    if (floor(mi) == mi) {
        dimS = d;
    } else {
        dimS = pow(2, floor(mi) + 1);
    }
    if (dimS < wSize) {
        dimS = wSize;
    }
    
    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 4;
    k_dim.y = 1;
    k_dim.z = 1;
    k_type = CNRT_FUNC_TYPE_UNION1;

    rmsNormUnion1<T><<<k_dim, k_type, queue>>>(y_, x_, w_, strideSrc, strideDest, shape, n, d, dimS, eps);
    cnrtQueueSync(queue);
}

void rmsNorm_fp16(cnrtQueue_t queue, void *y, void const *x, void const *w, int *strideSrc, int *strideDest, int *shape, int n, int d, float eps) {
   rmsNorm<half>(queue, y, x, w, strideSrc, strideDest, shape, n, d, eps);
}

void rms_norm_bang_f16(Tensor y, Tensor x, Tensor w, float epsilon, void *stream) {
    ASSERT_EQ(y.layout->ndim, 2);
    ASSERT_EQ(x.layout->ndim, 2);
    ASSERT_EQ(w.layout->ndim, 1);

    auto n = y.layout->shape[0],
         d = y.layout->shape[1];

    ASSERT_EQ(x.layout->shape[0], n);
    ASSERT_EQ(x.layout->shape[1], d);
    ASSERT_EQ(w.layout->shape[0], d);

    int ndim = y.layout->ndim;
    int x_stride[ndim], y_stride[ndim], shape[ndim];
    for (int i = 0; i < ndim; i++) {
        x_stride[i] = static_cast<int>(x.layout->strides[i]);
        y_stride[i] = static_cast<int>(y.layout->strides[i]);
        shape[i] = static_cast<int>(y.layout->shape[i]);
    }    

    auto queue = reinterpret_cast<cnrtQueue_t>(stream);
    rmsNorm_fp16(queue, y.data, x.data, w.data, x_stride, y_stride, shape, n, d, epsilon);
} 
