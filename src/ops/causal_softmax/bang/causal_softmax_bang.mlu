#include "bang.h"
#include "bang_device_functions.h"
#include "cnrt.h"
#include "causal_softmax_bang.h"
#include "../../../devices/bang/common_bang.h"
const int SRC_MAX_SIZE = 1024 * 64;//至少大于等于128字节
template <typename T>
__mlu_device__ void maskSoftmaxKernel(T *destination, T const *source, int *strideSrc, int *strideDest, int *shape, int othersize, int dimsize, int dimS, int mask, int ndim){
    const int nramNum = NRAM_MAX_SIZE/sizeof(T);
    __nram__  T nram_buffer[nramNum];
    const int maxNum = SRC_MAX_SIZE/sizeof(T);
    int wSize = 128 / sizeof(T);
    __nram__ T srcMax[2];
    if(dimsize > maxNum){
        T *src = nram_buffer;//[maxNum]
        T *destSum = src + maxNum;//[maxNum]
        T *destSumFinal = destSum + maxNum;//[wSize]
        T destOldMax;
        T destNewMax;

        int remain = dimsize % maxNum;
        int repeat = (dimsize - remain) / maxNum;
        int tidS;
        int tidD;

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);
        for(int i = indStart; i < indStart + step; i++){
            int inds = 0;
            int indd = 0;
            int indi = i;
            for (int j = ndim - 2; j >= 0; --j) {
                inds += (indi % shape[j]) * strideSrc[j];
                indd += (indi % shape[j]) * strideDest[j];
                indi /= shape[j];
            }
            __bang_write_zero(destSum, maxNum);
            __bang_write_zero(destSumFinal, wSize);
            destOldMax = -INFINITY;
            destNewMax = -INFINITY;
            for(int s = 0; s < repeat; s++){
                __bang_write_zero(destSum, wSize);
                tidS = inds + s * maxNum;
                __memcpy(src, source + tidS, maxNum * sizeof(T), GDRAM2NRAM);
                __bang_argmax(srcMax, src, maxNum);
                if(destNewMax < srcMax[0]){
                    destNewMax = srcMax[0];
                }
                __bang_sub_scalar(src, src, destNewMax, maxNum);
                __bang_active_exp_less_0(src, src, maxNum);
                if(s > 0){
                    __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                }
                __bang_add(destSum, destSum, src, maxNum);
                destOldMax = destNewMax;
            }
            if(remain){
                __bang_write_value(src, maxNum, -INFINITY);
                tidS = inds + repeat * maxNum;
                __memcpy(src, source + tidS, remain * sizeof(T), GDRAM2NRAM);
                if(destNewMax < srcMax[0]){
                    destNewMax = srcMax[0];
                }
                __bang_sub_scalar(src, src, destNewMax, maxNum);
                __bang_active_exp_less_0(src, src, maxNum);
                if(repeat > 0){
                    __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                }
                __bang_add(destSum, destSum, src, maxNum);
                destOldMax = destNewMax;
            }
            int segNum = maxNum / wSize;//准备数值求和
            for(int strip = segNum / 2; strip > 0; strip = strip / 2){
                for(int j = 0; j < strip; j++){
                    __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                }
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
            T globalSumInv = 1.0 / destSumFinal[0];//下面开始指数变换，写回GDRAM
            for(int s = 0; s < repeat; s++){
                
                tidS = inds + s * maxNum;
                __memcpy(src, source + tidS, maxNum * sizeof(T), GDRAM2NRAM);
                
                __bang_sub_scalar(src, src, destNewMax, maxNum);
                __bang_active_exp_less_0(src, src, maxNum);
                __bang_mul_scalar(src, src, globalSumInv, maxNum);
                tidD = indd + s * maxNum;
                __memcpy(destination + tidD, src, maxNum * sizeof(T), NRAM2GDRAM);
            }
            if(remain){
                tidS = inds + repeat * maxNum;
                __memcpy(src, source + tidS, remain * sizeof(T), GDRAM2NRAM);
                
                __bang_sub_scalar(src, src, destNewMax, maxNum);
                __bang_active_exp_less_0(src, src, maxNum);
                __bang_mul_scalar(src, src, globalSumInv, maxNum);
                tidD = indd + repeat * maxNum;
                __memcpy(destination + tidD, src, remain * sizeof(T), NRAM2GDRAM);
            }
        }
    }
    else{
        T *src = nram_buffer;//[dimsize]
        T *destSum = src + dimsize;//[dimS]
        T *destSumFinal = destSum + dimS;//[wSize]

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);
        __bang_write_zero(src, dimsize);
        __bang_write_zero(destSum, dimS);
        for(int i = indStart; i < indStart + step; i++){
            int inds = 0;
            int indd = 0;
            int indi = i ;
            for (int j = ndim - 2; j >= 0; --j) {
                inds += (indi % shape[j]) * strideSrc[j];
                indd += (indi % shape[j]) * strideDest[j];
                indi /= shape[j];
            }
            __memcpy(src, source + inds, dimsize * sizeof(T), GDRAM2NRAM);
            __memcpy(destSum, src, dimsize * sizeof(T), NRAM2NRAM);
            __bang_argmax(srcMax, src, dimsize);
            __bang_sub_scalar(destSum, destSum, srcMax[0], dimsize);

            int segNum = dimS / wSize;//准备数值求和
            for(int strip = segNum / 2; strip > 0; strip = strip / 2){
                for(int j = 0; j < strip; j++){
                    __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                }
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
            T globalSumInv = 1.0 / destSumFinal[0];//下面开始指数变换，写回GDRAM
            __bang_sub_scalar(src, src, srcMax[0], dimsize);
            __bang_mul_scalar(src, src, globalSumInv, dimsize);
            __memcpy(destination + indd, src, dimsize * sizeof(T), NRAM2GDRAM);
        }
    }
}
template<typename T>
__mlu_global__ void maskSoftmaxUnion1(T *destination, T const *source, int *strideSrc, int *strideDest, int *shape, int othersize, int dimsize, int dimS, int mask, int ndim) {

    maskSoftmaxKernel<T>(destination, source, strideSrc, strideDest, shape, othersize, dimsize, dimS, mask, ndim);
}
template<typename T>
void maskSoftmax(cnrtQueue_t queue, void *destination, void const *source, int *strideSrc, int *strideDest, int *shape, int othersize, int dimsize, int mask, int ndim) {
    int wSize = 128 / sizeof(T);
    auto y_ = reinterpret_cast<T *>(destination);
    auto x_ = reinterpret_cast<T const *>(source);
    

    int dimS;
    float mi = log2(dimsize);
    if (floor(mi) == mi) {
        dimS = dimsize;
    } else {
        dimS = pow(2, floor(mi) + 1);
    }
    if (dimS < wSize) {
        dimS = wSize;
    }
    
    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 4;
    k_dim.y = 1;
    k_dim.z = 1;
    k_type = CNRT_FUNC_TYPE_UNION1;

    maskSoftmaxUnion1<T><<<k_dim, k_type, queue>>>(y_, x_, strideSrc, strideDest, shape, othersize, dimsize, dimS, mask, ndim);
    cnrtQueueSync(queue);
}
void maskSoftmax_fp16(cnrtQueue_t queue, void *destination, void const *source, int *strideSrc, int *strideDest, int *shape, int othersize, int dimsize, int mask, int ndim) {
   maskSoftmax<half>(queue, destination, source, strideSrc, strideDest, shape, othersize, dimsize, mask, ndim);
}
void maskSoftmax_bang_f16(Tensor y, Tensor x, void *stream) {
    ASSERT_EQ(y.layout->ndim, 2);
    ASSERT_EQ(x.layout->ndim, 2);
    

    auto n = y.layout->shape[0],
         d = y.layout->shape[1];

    ASSERT_EQ(x.layout->shape[0], n);
    ASSERT_EQ(x.layout->shape[1], d);
   

    int ndim = y.layout->ndim;
    
    int x_stride[ndim], y_stride[ndim], shape[ndim];
    for (int i = 0; i < ndim; i++) {
        x_stride[i] = static_cast<int>(x.layout->strides[i]) / y.layout->dt.size;
        y_stride[i] = static_cast<int>(y.layout->strides[i]) / y.layout->dt.size;
        shape[i] = static_cast<int>(y.layout->shape[i]);
    }    
    int mask = shape[ndim - 1] - shape[ndim - 2];
    int *mlu_strideX, *mlu_strideY, *mlu_shape;
    CNRT_CHECK(cnrtMalloc((void **)&mlu_strideX, ndim * sizeof(int)));
    CNRT_CHECK(cnrtMalloc((void **)&mlu_strideY, ndim * sizeof(int)));
    CNRT_CHECK(cnrtMalloc((void **)&mlu_shape, ndim * sizeof(int)));
    CNRT_CHECK(cnrtMemcpy(mlu_strideX, x_stride, ndim * sizeof(int), cnrtMemcpyHostToDev));
    CNRT_CHECK(cnrtMemcpy(mlu_strideY, y_stride, ndim * sizeof(int), cnrtMemcpyHostToDev));
    CNRT_CHECK(cnrtMemcpy(mlu_shape, shape, ndim * sizeof(int), cnrtMemcpyHostToDev));
    auto queue = reinterpret_cast<cnrtQueue_t>(stream);
    maskSoftmax_fp16(queue, y.data, x.data, mlu_strideX, mlu_strideY, mlu_shape, n, d, mask, ndim);
    cnrtFree(mlu_strideX);
    cnrtFree(mlu_strideY);
    cnrtFree(mlu_shape);
} 